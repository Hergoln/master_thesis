{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_libs import get_network, DiffusionModel, scale_dataset\n",
    "from samples_generators import fill_vocabulary_c_v3, convert_back_to_code_c_v3, vocabulary_c_v3\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_vocabulary_c_v3()\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "\n",
    "# architecture\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "embedding_min_frequency = 1.0\n",
    "\n",
    "# optimization\n",
    "batch_size = 16\n",
    "ema = 0.999\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# dictionary related\n",
    "DICTIONARY_SIZE = len(vocabulary_c_v3)\n",
    "TOKENS_CAPACITY = 256\n",
    "\n",
    "widths = [64, 64, 96, 128, 256]\n",
    "block_depth = 2\n",
    "\n",
    "lang_base = f\"E:\\Studies\\master_thesis\\diffusion_models_checpoints_savespace\\\\final_checkpoints\\simple_c_v3\"\n",
    "model_path = f\"{lang_base}\\cp-0128\\model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1cfa67c7e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = get_network(\n",
    "      TOKENS_CAPACITY, embedding_min_frequency, embedding_max_frequency, \n",
    "      embedding_dims, widths=widths, block_depth=block_depth, name=\"complicated\"\n",
    "  )\n",
    "\n",
    "model = DiffusionModel(\n",
    "      TOKENS_CAPACITY, DICTIONARY_SIZE, network, batch_size, max_signal_rate, \n",
    "      min_signal_rate, ema, False\n",
    "  )\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.experimental.Adam(\n",
    "        learning_rate=learning_rate\n",
    "    ),\n",
    "    loss = keras.losses.mean_absolute_error\n",
    ")\n",
    "\n",
    "#normalizer\n",
    "n_w = np.load(f\"{lang_base}\\\\normalizer_weights.npy\", allow_pickle=True)\n",
    "normalizer = keras.layers.Normalization(mean=n_w[0], variance=n_w[1])\n",
    "normalizer.build((TOKENS_CAPACITY))\n",
    "model.normalizer = normalizer\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First should generate some sample to see if it still generates something that looks like code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "void ID0 ( char ID1 , char ID2 ) {\n",
      " int ID3 ;\n",
      " int ID4 = STRING + STRING ;\n",
      " char ID5 ;\n",
      " int ID6 = ID4 / STRING + NUM ;\n",
      " char ID9 ( ID6 | STRING + ID5 + STRING ;\n",
      " ID3 = ID3 / NUM - ID2 + STRING ;\n",
      " ID3 = STRING ;\n",
      " printf ( STRING , NUM ) ;\n",
      " ID4 = STRING + STRING + NUM ;\n",
      " for ( int ID9 = NUM ;\n",
      " ID9 , = NUM ;\n",
      " ID8 + - ) {\n",
      " char ID9 ;\n",
      " char ID9 ;\n",
      " int ID10 = STRING ;\n",
      " }\n",
      " return NUM ;\n",
      " }\n",
      " EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY EMPTY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw, denormalized = model.generate(1,80)\n",
    "for sample in denormalized:\n",
    "  scaled = scale_dataset(sample, DICTIONARY_SIZE)\n",
    "  print(\" \".join(convert_back_to_code_c_v3(scaled)).replace(\";\", \";\\n\").replace(\"{\", \"{\\n\").replace(\"}\", \"}\\n\"))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 2\n",
    "scaled = scale_dataset(denormalized[no], DICTIONARY_SIZE)\n",
    "convs = convert_back_to_code_c_v3(scaled)\n",
    "n = 5\n",
    "chunked = [convs[i:i + n] for i in range(0, len(convs), n)]\n",
    "for symbols in chunked:\n",
    "  print(\" \".join(symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.around(denormalized[no], 3).tolist()\n",
    "chunked = [l[i:i + n] for i in range(0, len(l), n)]\n",
    "for symbols in chunked:\n",
    "  print(np.around(np.asarray(symbols), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80d717405e3ce327cfc6a10a7c3be3a4a39215d9a17073dca3e8efe1a3c7fabb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
