{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple mathematical language model checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = ['-', '+', '/', '*', '^']\n",
    "EMPTY = 'EMPTY'\n",
    "NUM = 'NUM'\n",
    "EQ_SIGN = '='\n",
    "sl_vocabulary = ops + [EQ_SIGN, NUM, EMPTY] # has to be in this order, DO NOT TOUCH\n",
    "dictionary = {el:idx for idx,el in enumerate(sl_vocabulary)}\n",
    "\n",
    "def sl_gen_sample(length, max_vector_len):\n",
    "  values = []\n",
    "  for _ in range(length):\n",
    "    values.append(dictionary[NUM])\n",
    "    values.append(random.randint(0, len(ops)-1))\n",
    "  values += [dictionary[NUM],dictionary[EQ_SIGN],dictionary[NUM]]\n",
    "  filled_to_the_brim = values + [dictionary[EMPTY] for _ in range(max_vector_len - len(values))]\n",
    "  return filled_to_the_brim\n",
    "\n",
    "def sl_generate_samples(num_samples, min_len, max_len, max_vector_len):\n",
    "  lens = np.random.randint(min_len, max_len, num_samples)\n",
    "  return np.asarray([sl_gen_sample(l, max_vector_len) for l in lens])\n",
    "\n",
    "def sl_decode_sample(sample):\n",
    "  return [sl_vocabulary[int(v + 0.5)] for v in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_record(parsed_file):\n",
    "  parsed = np.loadtxt(parsed_file, dtype=float, converters=float)\n",
    "  return parsed\n",
    "\n",
    "def load_dataset(parsed_dir, max_size=None) -> list:\n",
    "  parsed_files = sorted(os.listdir(parsed_dir))\n",
    "  if max_size:\n",
    "    parsed_files = parsed_files[:max_size]\n",
    "\n",
    "  with ThreadPool() as pool:\n",
    "    parsed_files = pool.map(lambda f: f\"{parsed_dir}{f}\", parsed_files)\n",
    "    files = list(parsed_files)\n",
    "\n",
    "  with ThreadPool() as pool:\n",
    "  #   # pool.map guaranteese to preserve order\n",
    "  #   # pool.map 'consumes' mapping created in previous with block\n",
    "  #   # map() function returns a generator that is exhausted after is it used\n",
    "    return [np.array(pool.map(lambda file: prepare_record(file), files)), files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset_down(dataset, dic_size):\n",
    "    l = lambda x: (x / (dic_size - 1))\n",
    "    return np.array(list(map(l, dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe, dic_size):\n",
    "    l = lambda x: (x) * (dic_size - 1)\n",
    "    return np.array(list(map(l, dataframe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x, embedding_min_frequency, embedding_max_frequency, embedding_dims):\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims // 2,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=1\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(tokens_capacity, embedding_min_frequency, embedding_max_frequency, embedding_dims):\n",
    "    noisy_images = keras.Input(shape=(tokens_capacity))\n",
    "    noise_variances = keras.Input(shape=(1))\n",
    "\n",
    "    emb = lambda x: sinusoidal_embedding(x, embedding_min_frequency, embedding_max_frequency, embedding_dims)\n",
    "    e = layers.Lambda(emb)(noise_variances)\n",
    "\n",
    "    x = layers.Dense(1024)(noisy_images)\n",
    "    x = layers.Concatenate()([x, e])\n",
    "    x = layers.Dense(512, name=\"dense01\")(x)\n",
    "    x = layers.Dense(1024, name=\"dense02\", activation=keras.activations.relu)(x)\n",
    "    x = layers.Dense(2048, name=\"dense03\", activation=keras.activations.relu)(x)\n",
    "    x = layers.Dense(tokens_capacity, name=\"last_dense\")(x)\n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances], x, name=\"simple_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(\n",
    "      self, tokens_capacity, dictionary_size, network,\n",
    "      max_signal_rate, min_signal_rate,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokens_capacity = tokens_capacity\n",
    "        self.dictionary_size = dictionary_size\n",
    "        self.network = network\n",
    "        self.max_signal_rate = max_signal_rate\n",
    "        self.min_signal_rate = min_signal_rate\n",
    "        self.normalizer = layers.Normalization()\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\") # for training\n",
    "        self.sample_loss_tracker = keras.metrics.Mean(name=\"i_loss\") # for human evaluation\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.sample_loss_tracker]\n",
    "\n",
    "    def denormalize(self, samples):\n",
    "        samples = self.normalizer.mean + samples * self.normalizer.variance**0.5\n",
    "        return samples\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(self.max_signal_rate)\n",
    "        end_angle = tf.acos(self.min_signal_rate)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        # angles -> signal and noise rates\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_samples, noise_rates, signal_rates, training):\n",
    "        # the exponential moving average weights are used at evaluation\n",
    "        network = self.network\n",
    "\n",
    "        # predict noise component and calculate the sample component using it\n",
    "        pred_noises = network([noisy_samples, noise_rates**2], training=training)\n",
    "        pred_samples = (noisy_samples - noise_rates * pred_noises) / signal_rates # maybe some more sophisticated way of removing noise\n",
    "\n",
    "        return pred_noises, pred_samples\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        # reverse diffusion = sampling\n",
    "        num_samples = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy sample\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_samples = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_samples = next_noisy_samples\n",
    "\n",
    "            # separate the current noisy sample to its components\n",
    "            diffusion_times = tf.ones((num_samples)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            signal_rates = tf.expand_dims(signal_rates, axis=1)\n",
    "            noise_rates = tf.expand_dims(noise_rates, axis=1)\n",
    "\n",
    "            pred_noises, pred_samples = self.denoise(\n",
    "                noisy_samples, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_signal_rates = tf.expand_dims(next_signal_rates, axis=1)\n",
    "            next_noise_rates = tf.expand_dims(next_noise_rates, axis=1)\n",
    "            next_noisy_samples = (\n",
    "                next_signal_rates * pred_samples + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy sample will be used in the next step\n",
    "\n",
    "        return pred_samples\n",
    "\n",
    "    # generated values should be between 0 and 1, network should work better this way\n",
    "    def generate(self, num_samples, diffusion_steps):\n",
    "        # Generate sample from complete noise\n",
    "        initial_noise = tf.random.normal(shape=(num_samples, self.tokens_capacity))\n",
    "        generated_sample = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        denormalized_generated_sample = self.denormalize(generated_sample)\n",
    "        return generated_sample, tf.clip_by_value(tf.math.abs(denormalized_generated_sample), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from samples, I deducted that cp-0511 is a little bit better model for some unknown reason\n",
    "checkpoint_path = \"simplest_language_model\\\\cp-0511\\\\model\"\n",
    "\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "    \n",
    "# architecture\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "embedding_min_frequency = 1.0\n",
    "\n",
    "# dictionary related\n",
    "SIMPLE_MATH_DICTIONARY_SIZE = 8 # 246\n",
    "TOKENS_CAPACITY = 128 # 2048\n",
    "\n",
    "data_dir = f\"../data/simplest_language_model/\"\n",
    "dataset, filenames = load_dataset(data_dir)\n",
    "dataset = scale_dataset_down(dataset, SIMPLE_MATH_DICTIONARY_SIZE)\n",
    "    \n",
    "network = get_network(TOKENS_CAPACITY, embedding_min_frequency, embedding_max_frequency, embedding_dims)\n",
    "network.summary()\n",
    "print(\"Network created\")\n",
    "\n",
    "model = DiffusionModel(TOKENS_CAPACITY, SIMPLE_MATH_DICTIONARY_SIZE, network, max_signal_rate, min_signal_rate)\n",
    "print(\"Model created\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.experimental.AdamW(learning_rate=0, weight_decay=0),\n",
    "    loss = keras.losses.mean_absolute_error\n",
    ")\n",
    "model.normalizer.adapt(dataset)\n",
    "\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, denormalized = model.generate(5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.printoptions(precision=3, floatmode='fixed'):\n",
    "    for idx in range(len(denormalized)):\n",
    "      print(raw[idx])\n",
    "      print(denormalized[idx])\n",
    "      scaled = scale_dataset(denormalized[idx], SIMPLE_MATH_DICTIONARY_SIZE)\n",
    "      print(\" \".join(sl_decode_sample(scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_things(data_path):\n",
    "  '''\n",
    "    data will always have epoch number as its first column\n",
    "  '''\n",
    "  data = np.genfromtxt(data_path, dtype=float, delimiter=',', names=True)\n",
    "  labels = data.dtype.names\n",
    "\n",
    "  fig, ax = plt.subplots(len(labels) - 1, 1)\n",
    "\n",
    "  for count, label in enumerate(labels[1:]):\n",
    "    ax[count].set_title(label)\n",
    "    ax[count].plot(data[label])\n",
    "\n",
    "  fig.tight_layout()\n",
    "  return fig\n",
    "\n",
    "path_to_csv = \"simplest_language_model\\\\history.csv\"\n",
    "fig = plot_things(path_to_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80d717405e3ce327cfc6a10a7c3be3a4a39215d9a17073dca3e8efe1a3c7fabb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
