{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL simple language model checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['COL_ZERO', 'COL_TWO', 'COL_THREE', 'COL_FOUR', 'COL_FIVE', 'COL_SIX', 'COL_SEVEN']\n",
    "frm = 'FROM'\n",
    "\n",
    "literals = ['STRING', 'NUM']\n",
    "concats = ['AND', 'OR']\n",
    "\n",
    "sql_s_vocabulary = ['SELECT'] + cols + [',',\n",
    "    'FROM',\n",
    "    'TABLE_NAME',\n",
    "    'WHERE',\n",
    "    'IN',\n",
    "    'EXISTS',\n",
    "    # 'JOIN', # joins introduce a lot of complexity, will try without it and if it goes well add it later\n",
    "    'AND',\n",
    "    'OR',\n",
    "    'LIKE',\n",
    "    '(',\n",
    "    'STRING',\n",
    "    'NUM',\n",
    "    ')',\n",
    "    'LIMIT',\n",
    "    'EMPTY'\n",
    "  ]\n",
    "\n",
    "sqlSDict = {el:idx for idx,el in enumerate(sql_s_vocabulary)}\n",
    "\n",
    "global_recursive_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_col_not_in(in_cols):\n",
    "  cc = [it for it in cols if it not in in_cols]\n",
    "  return random.choice(cc)\n",
    "\n",
    "def gen_core():\n",
    "  q = []\n",
    "  q.append('SELECT')\n",
    "  c = cols[random.randint(0, len(cols)-1)]\n",
    "  output_cols = [c]\n",
    "  q.append(c)\n",
    "  temp_cols = [it for it in cols if it not in output_cols]\n",
    "  for cc in random.sample(temp_cols, k=random.randint(0, len(temp_cols)-1)):\n",
    "    q.append(',')\n",
    "    q.append(cc)\n",
    "    output_cols.append(cc)\n",
    "  q.append(frm)\n",
    "  q.append('TABLE_NAME')\n",
    "  return q, output_cols\n",
    "\n",
    "def gen_IN_conditions(in_cols):\n",
    "  if random.random() < 0.5:\n",
    "    val_type = 'STRING'\n",
    "  else:\n",
    "    val_type = 'NUM'\n",
    "  q = []\n",
    "  q.append(random.choice(in_cols))\n",
    "  q.append('IN')\n",
    "  q.append('(')\n",
    "  q.append(val_type)\n",
    "  for _ in range(random.randint(0, 16)):\n",
    "    q.append(',')\n",
    "    q.append(val_type)\n",
    "  q.append(')')\n",
    "  return q\n",
    "\n",
    "\n",
    "def gen_EXISTS_conditions(in_cols):\n",
    "  global global_recursive_counter\n",
    "  global_recursive_counter += 1\n",
    "  if global_recursive_counter > 8:\n",
    "    return gen_LIKE_conditions(in_cols)\n",
    "  q = []\n",
    "  q.append('EXISTS')\n",
    "  q.append('(')\n",
    "  q.append('SELECT')\n",
    "  q.append(random.choice(in_cols))\n",
    "  q.append(frm)\n",
    "  q.append('TABLE_NAME')\n",
    "  q.append('WHERE')\n",
    "  q += gen_condition(in_cols)\n",
    "  q.append(')')\n",
    "  global_recursive_counter = 0\n",
    "  return q\n",
    "\n",
    "\n",
    "def gen_LIKE_conditions(in_cols):\n",
    "  q = []\n",
    "  q.append(random.choice(in_cols))\n",
    "  q.append('LIKE')\n",
    "  q.append('STRING')\n",
    "  return q\n",
    "\n",
    "\n",
    "def gen_condition(in_cols):\n",
    "  possible_conditions = [[0.1, gen_IN_conditions], [0.11, gen_LIKE_conditions], [1.0, gen_EXISTS_conditions]]\n",
    "  rnJesus = random.random()\n",
    "  for condition in possible_conditions:\n",
    "    if rnJesus < condition[0]:\n",
    "      return condition[1](in_cols)\n",
    "\n",
    "\n",
    "def gen_conditions(in_cols):\n",
    "  q = []\n",
    "\n",
    "  q.append('WHERE')\n",
    "  q += gen_condition(in_cols)\n",
    "  return q\n",
    "\n",
    "\n",
    "def sql_simple_gen_sample(max_len_of_samples=None):\n",
    "  q, used_cols = gen_core()\n",
    "  q += gen_conditions(used_cols)\n",
    "  query = [sqlSDict[it] for it in q]\n",
    "  not_filled_query = [sqlSDict[it] for it in q]\n",
    "  if max_len_of_samples is not None:\n",
    "    empty_values = [sqlSDict['EMPTY'] for _ in range(max_len_of_samples - len(query))]\n",
    "    query += empty_values\n",
    "\n",
    "  return query, not_filled_query\n",
    "\n",
    "\n",
    "def sql_simple_generate_samples(num_of_samples, max_len_of_samples):\n",
    "  return [sql_simple_gen_sample(max_len_of_samples) for _ in range(num_of_samples)]\n",
    "\n",
    "\n",
    "def check_max_length():\n",
    "  lens = [len(sql_simple_gen_sample()) for _ in range(1000)]\n",
    "  return max(lens)\n",
    "\n",
    "def sql_simple_decode_sample(sample):\n",
    "  return [sql_s_vocabulary[int(v + 0.5)] for v in sample]\n",
    "\n",
    "\n",
    "def sql_simple_decode_sample_into_text(sample):\n",
    "  return \" \".join([sql_s_vocabulary[int(v + 0.5)] for v in sample])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_record(parsed_file):\n",
    "  parsed = np.loadtxt(parsed_file, dtype=float, converters=float)\n",
    "  return parsed\n",
    "\n",
    "def load_dataset(parsed_dir, max_size=None) -> list:\n",
    "  parsed_files = sorted(os.listdir(parsed_dir))\n",
    "  if max_size:\n",
    "    parsed_files = parsed_files[:max_size]\n",
    "\n",
    "  with ThreadPool() as pool:\n",
    "    parsed_files = pool.map(lambda f: f\"{parsed_dir}{f}\", parsed_files)\n",
    "    files = list(parsed_files)\n",
    "\n",
    "  with ThreadPool() as pool:\n",
    "  #   # pool.map guaranteese to preserve order\n",
    "  #   # pool.map 'consumes' mapping created in previous with block\n",
    "  #   # map() function returns a generator that is exhausted after is it used\n",
    "    return [np.array(pool.map(lambda file: prepare_record(file), files)), files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_normalization(compute, tokens_capacity, file, dataset):\n",
    "    if compute:\n",
    "        layer = keras.layers.Normalization()\n",
    "        layer.adapt(dataset)\n",
    "        w = layer.get_weights()\n",
    "        w = np.asarray(w[:-1])\n",
    "        np.save(file, w)\n",
    "        print(\"adaptet normalizer\")\n",
    "    else:\n",
    "        n_w = np.load(file, allow_pickle=True)\n",
    "        layer = keras.layers.Normalization(mean=n_w[0], variance=n_w[1])\n",
    "        print(\"loaded normalizer\")\n",
    "    layer.build((tokens_capacity))\n",
    "    return layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset_down(dataset, dic_size):\n",
    "    l = lambda x: (x / (dic_size - 1))\n",
    "    return np.array(list(map(l, dataset)))\n",
    "\n",
    "def scale_dataset(dataframe, dic_size):\n",
    "    l = lambda x: (x) * (dic_size - 1)\n",
    "    return np.array(list(map(l, dataframe)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x, embedding_min_frequency, embedding_max_frequency, embedding_dims):\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims //2,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=2\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(tokens_capacity, widths, block_depth, embedding_min_frequency, embedding_max_frequency, embedding_dims):\n",
    "    noisy_images = keras.Input(shape=(tokens_capacity))\n",
    "    noise_variances = keras.Input(shape=(1,1))\n",
    "\n",
    "    emb = lambda x: sinusoidal_embedding(x, embedding_min_frequency, embedding_max_frequency, embedding_dims)\n",
    "    e = layers.Lambda(emb)(noise_variances)\n",
    "    e = layers.UpSampling1D(tokens_capacity)(e)\n",
    "\n",
    "    x = layers.Reshape((tokens_capacity, 1))(noisy_images)\n",
    "    x = layers.Conv1D(widths[0], kernel_size=1)(x)\n",
    "    x = layers.Concatenate()([x, e])\n",
    "\n",
    "    skips = []\n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "    for width in reversed(widths[:-1]):\n",
    "        x = UpBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    x = layers.Conv1D(1, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    return keras.Model([noisy_images, noise_variances], x, name=\"residual_unet\")\n",
    "    \n",
    "\n",
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[2]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv1D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv1D(width, kernel_size=3, padding=\"same\", activation=keras.activations.swish)(x)\n",
    "        x = layers.Conv1D(width, kernel_size=3, padding=\"same\", activation=keras.activations.swish)(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling1D(pool_size=2, )(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "def UpBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling1D(size=2)(x)\n",
    "        for _ in range(block_depth):\n",
    "            x = layers.Concatenate()([x, skips.pop()])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(\n",
    "      self, tokens_capacity, dictionary_size, network,\n",
    "      max_signal_rate, min_signal_rate, normalizer\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokens_capacity = tokens_capacity\n",
    "        self.dictionary_size = dictionary_size\n",
    "        self.network = network\n",
    "        self.max_signal_rate = max_signal_rate\n",
    "        self.min_signal_rate = min_signal_rate\n",
    "        self.normalizer = normalizer # shouldn't have axis=None cause normalization will be then strongly affected by mostly EMPTY samples\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\") # for training\n",
    "        self.sample_loss_tracker = keras.metrics.Mean(name=\"i_loss\") # for human evaluation\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.noise_loss_tracker, self.sample_loss_tracker]\n",
    "\n",
    "    def denormalize(self, samples):\n",
    "        samples = self.normalizer.mean + samples * self.normalizer.variance**0.5\n",
    "        return samples\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion times -> angles\n",
    "        start_angle = tf.acos(self.max_signal_rate)\n",
    "        end_angle = tf.acos(self.min_signal_rate)\n",
    "\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "\n",
    "        # angles -> signal and noise rates\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "\n",
    "        # note that their squared sum is always: sin^2(x) + cos^2(x) = 1\n",
    "\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_samples, noise_rates, signal_rates, training):\n",
    "        # the exponential moving average weights are used at evaluation\n",
    "        network = self.network\n",
    "\n",
    "        # predict noise component and calculate the sample component using it\n",
    "        pred_noises = network([noisy_samples, noise_rates**2], training=training)\n",
    "        pred_samples = (noisy_samples - noise_rates * pred_noises) / signal_rates # maybe some more sophisticated way of removing noise\n",
    "\n",
    "        return pred_noises, pred_samples\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        # reverse diffusion = sampling\n",
    "        num_samples = initial_noise.shape[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy sample\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_samples = initial_noise\n",
    "        for step in range(diffusion_steps):\n",
    "            noisy_samples = next_noisy_samples\n",
    "\n",
    "            # separate the current noisy sample to its components\n",
    "            diffusion_times = tf.ones((num_samples)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            signal_rates = tf.expand_dims(signal_rates, axis=1)\n",
    "            noise_rates = tf.expand_dims(noise_rates, axis=1)\n",
    "\n",
    "            pred_noises, pred_samples = self.denoise(\n",
    "                noisy_samples, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_signal_rates = tf.expand_dims(next_signal_rates, axis=1)\n",
    "            next_noise_rates = tf.expand_dims(next_noise_rates, axis=1)\n",
    "            next_noisy_samples = (\n",
    "                next_signal_rates * pred_samples + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy sample will be used in the next step\n",
    "\n",
    "        return pred_samples\n",
    "\n",
    "    # generated values should be between 0 and 1, network should work better this way\n",
    "    def generate(self, num_samples, diffusion_steps):\n",
    "        # Generate sample from complete noise\n",
    "        initial_noise = tf.random.normal(shape=(num_samples, self.tokens_capacity))\n",
    "        generated_sample = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        denormalized_generated_sample = self.denormalize(generated_sample)\n",
    "        return generated_sample, tf.clip_by_value(tf.math.abs(denormalized_generated_sample), 0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from samples, I deducted that cp-0511 is a little bit better model for some unknown reason\n",
    "checkpoint_path = \"sql_simple_language_model\\\\cp-0032\"\n",
    "model_path = f\"{checkpoint_path}\\\\model\"\n",
    "normalizer_path = f'sql_simple_language_model\\\\normalizer_weights.npy'\n",
    "# sampling\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "\n",
    "# architecture\n",
    "embedding_dims = 32\n",
    "embedding_max_frequency = 1000.0\n",
    "embedding_min_frequency = 1.0\n",
    "\n",
    "# optimization\n",
    "batch_size = 16\n",
    "\n",
    "# dictionary related\n",
    "SIMPLE_SQL_DICTIONARY_SIZE = 23\n",
    "TOKENS_CAPACITY = 128\n",
    "\n",
    "widths = [64, 64, 96, 128]\n",
    "block_depth = 2\n",
    "    \n",
    "network = get_network(TOKENS_CAPACITY, widths, block_depth, embedding_min_frequency, embedding_max_frequency, embedding_dims)\n",
    "network.summary()\n",
    "print(\"Network created\")\n",
    "normalizer = resolve_normalization(False, tokens_capacity=TOKENS_CAPACITY, file=normalizer_path, dataset=None)\n",
    "model = DiffusionModel(TOKENS_CAPACITY, SIMPLE_SQL_DICTIONARY_SIZE, network, max_signal_rate, min_signal_rate, normalizer)\n",
    "print(\"Model created\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.experimental.AdamW(learning_rate=0, weight_decay=0),\n",
    "    loss = keras.losses.mean_absolute_error\n",
    ")\n",
    "\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, denormalized = model.generate(5,10)\n",
    "for sample in denormalized:\n",
    "  scaled = scale_dataset(sample, SIMPLE_SQL_DICTIONARY_SIZE)\n",
    "  print(\" \".join(sql_simple_decode_sample(scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_things(data_path):\n",
    "  '''\n",
    "    data will always have epoch number as its first column\n",
    "  '''\n",
    "  data = np.genfromtxt(data_path, dtype=float, delimiter=',', names=True)\n",
    "  labels = data.dtype.names\n",
    "\n",
    "  fig, ax = plt.subplots(len(labels) - 2, 1)\n",
    "\n",
    "  for count, label in enumerate(labels[1:-1]):\n",
    "    ax[count].set_title(label)\n",
    "    ax[count].plot(data[label])\n",
    "\n",
    "  fig.tight_layout()\n",
    "  return fig\n",
    "\n",
    "path_to_csv = \"sql_simple_language_model\\\\history.csv\"\n",
    "fig = plot_things(path_to_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80d717405e3ce327cfc6a10a7c3be3a4a39215d9a17073dca3e8efe1a3c7fabb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
